{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import torch\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from nltk.corpus import words as nltk_words\n",
    "\n",
    "# def load_words_with_nltk(filepath, output_filepath=\"words_nltk_filtered.txt\"):\n",
    "#     \"\"\"\n",
    "#     Loads words, cleans them, and uses the NLTK library's word corpus\n",
    "#     to filter for real English words. The cleaned list is then saved.\n",
    "#     \"\"\"\n",
    "#     # Create a set of valid English words from the NLTK corpus for fast lookups.\n",
    "#     print(\"Loading NLTK English word corpus...\")\n",
    "#     valid_english_words = set(nltk_words.words())\n",
    "    \n",
    "#     with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#         word_list = f.readlines()\n",
    "\n",
    "#     # --- Step 1: Initial Cleaning ---\n",
    "#     print(\"Performing initial cleaning...\")\n",
    "#     cleaned_words = [re.sub(r'[^a-z]', '', word.lower().strip()) for word in word_list]\n",
    "#     unique_words = set(word for word in cleaned_words if len(word) > 1)\n",
    "    \n",
    "#     # --- Step 2: Filter Using the NLTK Word List ---\n",
    "#     print(f\"Checking {len(unique_words)} unique words against the NLTK dictionary...\")\n",
    "#     final_words = sorted([word for word in unique_words if word in valid_english_words])\n",
    "    \n",
    "#     # --- Step 3: Save the Final Words to a File ---\n",
    "#     print(f\"\\nFound {len(final_words)} real words. Saving them to {output_filepath}...\")\n",
    "#     with open(output_filepath, 'w', encoding='utf-8') as f_out:\n",
    "#         for word in final_words:\n",
    "#             f_out.write(word + \"\\n\")\n",
    "#     print(\"File saved successfully.\")\n",
    "\n",
    "#     return final_words\n",
    "\n",
    "# # --- How to use it ---\n",
    "# # This will create your new, highly accurate word list for training.\n",
    "# DATA_PATH = 'words_250000_train.txt'\n",
    "# filtered_list = load_words_with_nltk(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import collections\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "def load_words(filepath):\n",
    "    \"\"\"Loads and cleans words from a file.\"\"\"\n",
    "    # Create a dummy file if it doesn't exist for demonstration\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File '{filepath}' not found. Creating a dummy word file.\")\n",
    "        dummy_words = ['apple', 'banana', 'orange', 'grape', 'strawberry', 'blueberry', 'python', 'pytorch', 'tensor', 'notebook']\n",
    "        with open(filepath, 'w') as f:\n",
    "            for word in dummy_words:\n",
    "                f.write(word + '\\n')\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        words = f.readlines()\n",
    "    # Clean words: lowercase, strip whitespace, remove non-alpha characters\n",
    "    words = [re.sub(r'[^a-z]', '', word.lower().strip()) for word in words]\n",
    "    # Filter out very short words or empty strings resulting from cleaning\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    return words\n",
    "\n",
    "def create_char_mappings():\n",
    "    \"\"\"Creates character to index and index to character mappings.\"\"\"\n",
    "    all_chars = string.ascii_lowercase + '_#' # '#' is our padding token\n",
    "    char_to_idx = {char: i for i, char in enumerate(all_chars)}\n",
    "    idx_to_char = {i: char for i, char in enumerate(all_chars)}\n",
    "    vocab_size = len(all_chars)\n",
    "    mask_token_idx = char_to_idx['_']\n",
    "    padding_idx = char_to_idx['#']\n",
    "    return char_to_idx, idx_to_char, vocab_size, mask_token_idx, padding_idx\n",
    "\n",
    "# --- 2. Dataset and Dataloader Preparation ---\n",
    "\n",
    "class MaskedWordDataset(Dataset):\n",
    "    \"\"\"Generates a randomly masked version of a word on-the-fly.\"\"\"\n",
    "    def __init__(self, word_list, char_to_idx, mask_token_idx):\n",
    "        self.word_list = word_list\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.mask_token_idx = mask_token_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word_list[idx]\n",
    "        word_indices = torch.tensor([self.char_to_idx[char] for char in word], dtype=torch.long)\n",
    "        n = len(word)\n",
    "        if n <= 2:\n",
    "            mask_positions = [random.randint(0, n - 1)]\n",
    "        else:\n",
    "            num_masks = random.randint(1, max(1, n - 1))\n",
    "            mask_positions = sorted(random.sample(range(n), num_masks))\n",
    "        input_word = word_indices.clone()\n",
    "        input_word[mask_positions] = self.mask_token_idx\n",
    "        target_word = torch.full_like(word_indices, -100) # ignore_index\n",
    "        target_word[mask_positions] = word_indices[mask_positions]\n",
    "        return input_word, target_word\n",
    "\n",
    "def pad_collate_fn(batch, padding_value):\n",
    "    \"\"\"Pads sequences in a batch.\"\"\"\n",
    "    inputs, targets = zip(*batch)\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=padding_value)\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    return padded_inputs, padded_targets\n",
    "\n",
    "# --- 3. Unified Training Loop ---\n",
    "\n",
    "def train_model(model, dataloader, optimizer, scheduler, criterion, device, num_epochs, model_path):\n",
    "    \"\"\"Trains a given model with gradient clipping.\"\"\"\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    print(f\"\\n--- Starting Training for {model.__class__.__name__} ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[2]), targets.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({'Loss': loss.item()})\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} Completed | Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model improved and saved to {model_path} (Loss: {best_loss:.4f})\")\n",
    "    print(\"--- Training Finished ---\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining path of file*(change this to train the model on a different file)\n",
    "DATA_PATH = 'words_250000_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Class: Positional Encoding (Used by BiLSTM and Transformer) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# --- Model 1: BiLSTM Solver ---\n",
    "class BiLSTMSolver(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, padding_idx, dropout=0.3):\n",
    "        super(BiLSTMSolver, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "            bidirectional=True, batch_first=True, dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) * math.sqrt(self.embedding_dim)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        lstm_out, _ = self.lstm(pos_encoded)\n",
    "        return self.fc(lstm_out)\n",
    "\n",
    "# --- Model 2: Transformer Solver ---\n",
    "class TransformerSolver(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_heads, n_encoder_layers, dim_feedforward, padding_idx, dropout=0.3):\n",
    "        super(TransformerSolver, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_encoder_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_key_padding_mask = (src == self.padding_idx)\n",
    "        embedded = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        pos_encoded = self.pos_encoder(embedded)\n",
    "        transformer_out = self.transformer_encoder(pos_encoded, src_key_padding_mask=src_key_padding_mask)\n",
    "        return self.fc(transformer_out)\n",
    "\n",
    "# --- Model 3: CharCNN Solver ---\n",
    "class CharCNNSolver(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes, padding_idx, dropout=0.5):\n",
    "        super(CharCNNSolver, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # Repeat output for each position to maintain a consistent output shape\n",
    "        return self.fc(cat).unsqueeze(1).repeat(1, x.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for BiLSTMSolver ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 444/444 [01:34<00:00,  4.72it/s, Loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed | Average Loss: 2.4847\n",
      "Model improved and saved to best_bilstm_solver.pth (Loss: 2.4847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  35%|███▍      | 154/444 [00:33<01:02,  4.62it/s, Loss=2.28]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# --- Run Training ---\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbilstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH_BILSTM\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, scheduler, criterion, device, num_epochs, model_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     99\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m    102\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- BiLSTM Training Configuration ---\n",
    "MODEL_PATH_BILSTM = 'best_bilstm_solver.pth'\n",
    "TARGET_GPU_ID = 3\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 4\n",
    "DROPOUT = 0.4\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(f'cuda:{TARGET_GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "char_to_idx, _, vocab_size, mask_token_idx, padding_idx = create_char_mappings()\n",
    "all_words = load_words(DATA_PATH)\n",
    "train_dataset = MaskedWordDataset(all_words, char_to_idx, mask_token_idx)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: pad_collate_fn(b, padding_idx),\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# --- Model Initialization ---\n",
    "bilstm_model = BiLSTMSolver(\n",
    "    vocab_size, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, padding_idx, DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(bilstm_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3)\n",
    "\n",
    "# --- Run Training ---\n",
    "train_model(bilstm_model, train_dataloader, optimizer, scheduler, criterion, device, NUM_EPOCHS, MODEL_PATH_BILSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for TransformerSolver ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: 100%|██████████| 888/888 [02:05<00:00,  7.10it/s, Loss=2.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed | Average Loss: 2.8936\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8936)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Completed | Average Loss: 2.8765\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8765)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Completed | Average Loss: 2.8711\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8711)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Completed | Average Loss: 2.8676\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Completed | Average Loss: 2.8657\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8657)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Completed | Average Loss: 2.8649\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8649)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Completed | Average Loss: 2.8636\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8636)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Completed | Average Loss: 2.8619\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Completed | Average Loss: 2.8583\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Completed | Average Loss: 2.8560\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Completed | Average Loss: 2.8522\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8522)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Completed | Average Loss: 2.8506\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Completed | Average Loss: 2.8477\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Completed | Average Loss: 2.8452\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Completed | Average Loss: 2.8415\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Completed | Average Loss: 2.8359\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Completed | Average Loss: 2.8300\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Completed | Average Loss: 2.8215\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/60: 100%|██████████| 888/888 [02:05<00:00,  7.05it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Completed | Average Loss: 2.8119\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Completed | Average Loss: 2.8033\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.8033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Completed | Average Loss: 2.7953\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7953)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/60: 100%|██████████| 888/888 [02:05<00:00,  7.06it/s, Loss=2.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Completed | Average Loss: 2.7865\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Completed | Average Loss: 2.7769\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Completed | Average Loss: 2.7684\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Completed | Average Loss: 2.7591\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/60: 100%|██████████| 888/888 [02:06<00:00,  7.00it/s, Loss=2.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Completed | Average Loss: 2.7494\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7494)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Completed | Average Loss: 2.7387\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7387)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Completed | Average Loss: 2.7265\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Completed | Average Loss: 2.7126\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Completed | Average Loss: 2.7018\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.7018)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/60: 100%|██████████| 888/888 [02:06<00:00,  7.01it/s, Loss=2.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Completed | Average Loss: 2.6896\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Completed | Average Loss: 2.6780\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6780)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Completed | Average Loss: 2.6663\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6663)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Completed | Average Loss: 2.6539\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6539)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Completed | Average Loss: 2.6431\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6431)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Completed | Average Loss: 2.6296\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Completed | Average Loss: 2.6180\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/60: 100%|██████████| 888/888 [02:05<00:00,  7.05it/s, Loss=2.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Completed | Average Loss: 2.6045\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.6045)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Completed | Average Loss: 2.5885\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5885)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Completed | Average Loss: 2.5755\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Completed | Average Loss: 2.5608\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/60: 100%|██████████| 888/888 [02:05<00:00,  7.06it/s, Loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Completed | Average Loss: 2.5444\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5444)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Completed | Average Loss: 2.5315\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/60: 100%|██████████| 888/888 [02:06<00:00,  7.01it/s, Loss=2.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Completed | Average Loss: 2.5156\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5156)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Completed | Average Loss: 2.5011\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.5011)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Completed | Average Loss: 2.4859\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Completed | Average Loss: 2.4715\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4715)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/60: 100%|██████████| 888/888 [02:05<00:00,  7.05it/s, Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Completed | Average Loss: 2.4595\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4595)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/60: 100%|██████████| 888/888 [02:05<00:00,  7.06it/s, Loss=2.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Completed | Average Loss: 2.4441\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4441)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Completed | Average Loss: 2.4315\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Completed | Average Loss: 2.4184\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4184)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Completed | Average Loss: 2.4069\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.4069)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/60: 100%|██████████| 888/888 [02:05<00:00,  7.05it/s, Loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Completed | Average Loss: 2.3945\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Completed | Average Loss: 2.3829\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/60: 100%|██████████| 888/888 [02:05<00:00,  7.06it/s, Loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Completed | Average Loss: 2.3719\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3719)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Completed | Average Loss: 2.3642\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3642)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Completed | Average Loss: 2.3553\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3553)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/60: 100%|██████████| 888/888 [02:06<00:00,  7.03it/s, Loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Completed | Average Loss: 2.3474\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3474)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/60: 100%|██████████| 888/888 [02:06<00:00,  7.02it/s, Loss=2.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Completed | Average Loss: 2.3406\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3406)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/60: 100%|██████████| 888/888 [02:06<00:00,  7.04it/s, Loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Completed | Average Loss: 2.3347\n",
      "Model improved and saved to best_transformer_solver.pth (Loss: 2.3347)\n",
      "--- Training Finished ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Transformer Training Configuration ---\n",
    "MODEL_PATH_TRANSFORMER = 'best_transformer_solver.pth'\n",
    "TARGET_GPU_ID = 3 # Or a different GPU if you have one\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 256 # Transformers might need smaller batch sizes\n",
    "NUM_EPOCHS = 60\n",
    "EMBEDDING_DIM = 768\n",
    "N_HEADS = 12\n",
    "N_LAYERS = 6\n",
    "DIM_FEEDFORWARD = 1024\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(f'cuda:{TARGET_GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "char_to_idx, _, vocab_size, mask_token_idx, padding_idx = create_char_mappings()\n",
    "all_words = load_words(DATA_PATH)\n",
    "train_dataset = MaskedWordDataset(all_words, char_to_idx, mask_token_idx)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: pad_collate_fn(b, padding_idx),\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# --- Model Initialization ---\n",
    "transformer_model = TransformerSolver(\n",
    "    vocab_size, EMBEDDING_DIM, N_HEADS, N_LAYERS, DIM_FEEDFORWARD, padding_idx, DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3)\n",
    "\n",
    "# --- Run Training ---\n",
    "train_model(transformer_model, train_dataloader, optimizer, scheduler, criterion, device, NUM_EPOCHS, MODEL_PATH_TRANSFORMER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for CharCNNSolver ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 444/444 [00:22<00:00, 20.13it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed | Average Loss: 2.8944\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8944)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 444/444 [00:21<00:00, 20.18it/s, Loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Completed | Average Loss: 2.8586\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8586)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 444/444 [00:21<00:00, 20.27it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Completed | Average Loss: 2.8530\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8530)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 444/444 [00:21<00:00, 20.19it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Completed | Average Loss: 2.8481\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8481)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 444/444 [00:21<00:00, 20.21it/s, Loss=2.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Completed | Average Loss: 2.8446\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8446)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 444/444 [00:22<00:00, 20.18it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Completed | Average Loss: 2.8419\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 444/444 [00:22<00:00, 20.17it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Completed | Average Loss: 2.8409\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8409)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 444/444 [00:21<00:00, 20.19it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Completed | Average Loss: 2.8378\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8378)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 444/444 [00:21<00:00, 20.20it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Completed | Average Loss: 2.8355\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 444/444 [00:21<00:00, 20.23it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Completed | Average Loss: 2.8340\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8340)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 444/444 [00:21<00:00, 20.19it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Completed | Average Loss: 2.8337\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 444/444 [00:22<00:00, 20.05it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Completed | Average Loss: 2.8317\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8317)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 444/444 [00:22<00:00, 20.13it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Completed | Average Loss: 2.8311\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8311)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 444/444 [00:22<00:00, 20.06it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Completed | Average Loss: 2.8303\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8303)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 444/444 [00:22<00:00, 20.10it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Completed | Average Loss: 2.8275\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 444/444 [00:22<00:00, 20.09it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Completed | Average Loss: 2.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 444/444 [00:22<00:00, 20.09it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Completed | Average Loss: 2.8272\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 444/444 [00:22<00:00, 20.12it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Completed | Average Loss: 2.8267\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8267)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 444/444 [00:22<00:00, 20.03it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Completed | Average Loss: 2.8255\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 444/444 [00:22<00:00, 20.12it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Completed | Average Loss: 2.8248\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8248)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 444/444 [00:22<00:00, 20.09it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Completed | Average Loss: 2.8236\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8236)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 444/444 [00:22<00:00, 20.12it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Completed | Average Loss: 2.8227\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 444/444 [00:22<00:00, 20.11it/s, Loss=2.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Completed | Average Loss: 2.8216\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 444/444 [00:22<00:00, 20.03it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Completed | Average Loss: 2.8221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 444/444 [00:22<00:00, 20.08it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Completed | Average Loss: 2.8212\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8212)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 444/444 [00:22<00:00, 20.06it/s, Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Completed | Average Loss: 2.8222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 444/444 [00:22<00:00, 20.18it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Completed | Average Loss: 2.8210\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 444/444 [00:22<00:00, 20.07it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Completed | Average Loss: 2.8201\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 444/444 [00:22<00:00, 20.11it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Completed | Average Loss: 2.8193\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 444/444 [00:22<00:00, 20.16it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Completed | Average Loss: 2.8198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 444/444 [00:22<00:00, 20.13it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Completed | Average Loss: 2.8191\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8191)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 444/444 [00:22<00:00, 20.14it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Completed | Average Loss: 2.8189\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 444/444 [00:21<00:00, 20.23it/s, Loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Completed | Average Loss: 2.8191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 444/444 [00:21<00:00, 20.21it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Completed | Average Loss: 2.8176\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 444/444 [00:21<00:00, 20.21it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Completed | Average Loss: 2.8176\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 444/444 [00:22<00:00, 20.10it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Completed | Average Loss: 2.8175\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 444/444 [00:22<00:00, 20.12it/s, Loss=2.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Completed | Average Loss: 2.8168\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 444/444 [00:22<00:00, 20.14it/s, Loss=2.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Completed | Average Loss: 2.8154\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8154)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 444/444 [00:22<00:00, 20.16it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Completed | Average Loss: 2.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 444/444 [00:22<00:00, 20.12it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Completed | Average Loss: 2.8157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 444/444 [00:22<00:00, 20.11it/s, Loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Completed | Average Loss: 2.8149\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 444/444 [00:21<00:00, 20.21it/s, Loss=2.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Completed | Average Loss: 2.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 444/444 [00:21<00:00, 20.30it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Completed | Average Loss: 2.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 444/444 [00:21<00:00, 20.27it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Completed | Average Loss: 2.8154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 444/444 [00:21<00:00, 20.22it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Completed | Average Loss: 2.8134\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8134)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 444/444 [00:21<00:00, 20.19it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Completed | Average Loss: 2.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 444/444 [00:21<00:00, 20.21it/s, Loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Completed | Average Loss: 2.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 444/444 [00:21<00:00, 20.38it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Completed | Average Loss: 2.8124\n",
      "Model improved and saved to best_charcnn_solver.pth (Loss: 2.8124)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 444/444 [00:21<00:00, 20.34it/s, Loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Completed | Average Loss: 2.8131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 444/444 [00:21<00:00, 20.20it/s, Loss=2.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Completed | Average Loss: 2.8136\n",
      "--- Training Finished ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CharCNN Training Configuration ---\n",
    "MODEL_PATH_CHARCNN = 'best_charcnn_solver.pth'\n",
    "TARGET_GPU_ID = 3 # Or a different GPU if you have one\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "EMBEDDING_DIM = 768\n",
    "NUM_FILTERS = 128\n",
    "KERNEL_SIZES = [2, 3, 4, 5]\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device(f'cuda:{TARGET_GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "char_to_idx, _, vocab_size, mask_token_idx, padding_idx = create_char_mappings()\n",
    "all_words = load_words(DATA_PATH)\n",
    "train_dataset = MaskedWordDataset(all_words, char_to_idx, mask_token_idx)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: pad_collate_fn(b, padding_idx),\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# --- Model Initialization ---\n",
    "charcnn_model = CharCNNSolver(\n",
    "    vocab_size, EMBEDDING_DIM, NUM_FILTERS, KERNEL_SIZES, padding_idx, DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(charcnn_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3)\n",
    "\n",
    "# --- Run Training ---\n",
    "train_model(charcnn_model, train_dataloader, optimizer, scheduler, criterion, device, NUM_EPOCHS, MODEL_PATH_CHARCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import parse_qs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import BiLSTMSolver, TransformerSolver, CharCNNSolver\n",
    "\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        # --- Ensemble Weights ---\n",
    "        self.W_BILSTM = 0.5\n",
    "        self.W_TRANSFORMER = 0.3\n",
    "        self.W_CHARCNN = 0.2\n",
    "\n",
    "        # --- Shared Parameters & Device ---\n",
    "        self.char_to_idx, self.idx_to_char, self.vocab_size, self.mask_token_idx, self.padding_idx = self._create_char_mappings()\n",
    "        TARGET_GPU_ID = 3 # You can change this ID\n",
    "        self.device = self._get_device(TARGET_GPU_ID)\n",
    "\n",
    "        # --- Load All Three Models ---\n",
    "        print(\"--- Initializing Ensemble Hangman Solver ---\")\n",
    "        self.bilstm_model = self._load_model('bilstm')\n",
    "        self.transformer_model = self._load_model('transformer')\n",
    "        self.charcnn_model = self._load_model('charcnn')\n",
    "        print(\"--- Initialization Complete ---\")\n",
    "\n",
    "    def _create_char_mappings(self):\n",
    "        \"\"\"Creates character-to-index mappings for the models.\"\"\"\n",
    "        all_chars = string.ascii_lowercase + '_#' # '_' is mask, '#' is padding\n",
    "        char_to_idx = {char: i for i, char in enumerate(all_chars)}\n",
    "        idx_to_char = {i: char for i, char in enumerate(all_chars)}\n",
    "        return char_to_idx, idx_to_char, len(all_chars), char_to_idx['_'], char_to_idx['#']\n",
    "        \n",
    "    def _get_device(self, gpu_id):\n",
    "        \"\"\"Selects the appropriate device (GPU or CPU) for PyTorch models.\"\"\"\n",
    "        if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():\n",
    "            device = torch.device(f'cuda:{gpu_id}')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        print(f\"All models will be loaded onto device: {device}\")\n",
    "        return device\n",
    "\n",
    "    def _load_model(self, model_type):\n",
    "        \"\"\"Loads a specified pre-trained model.\"\"\"\n",
    "        MODEL_PATH = f'best_{model_type}_solver.pth'\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            print(f\"Warning: Model file not found at {MODEL_PATH}. This model will be skipped.\")\n",
    "            return None\n",
    "        try:\n",
    "            if model_type == 'bilstm':\n",
    "                model = BiLSTMSolver(self.vocab_size, 768, 512, 4, self.padding_idx, dropout=0.4)\n",
    "            elif model_type == 'transformer':\n",
    "                model = TransformerSolver(self.vocab_size, 768, 12, 6, 1024, self.padding_idx, dropout=0.2)\n",
    "            elif model_type == 'charcnn':\n",
    "                model = CharCNNSolver(self.vocab_size, 768, 128, [2,3,4,5], self.padding_idx, dropout=0.5)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "            \n",
    "            state_dict = torch.load(MODEL_PATH, map_location=self.device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            print(f\"Successfully loaded {model_type} model from {MODEL_PATH}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {model_type} model from {MODEL_PATH}. Reason: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_aggregated_prediction(self, model, input_tensor):\n",
    "        \"\"\"Gets the probability distribution for letters in blank spaces from a model.\"\"\"\n",
    "        if model is None: return None\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = torch.softmax(output, dim=2)[0]\n",
    "            \n",
    "            # Identify blank positions in the original word\n",
    "            clean_word = \"\".join([self.idx_to_char[idx.item()] for idx in input_tensor[0] if idx.item() != self.padding_idx])\n",
    "            blank_indices = [i for i, char in enumerate(clean_word) if char == '_']\n",
    "            \n",
    "            if not blank_indices: return torch.zeros(self.vocab_size, device=self.device)\n",
    "            \n",
    "            # Get probabilities only at blank positions\n",
    "            blank_probs = probabilities[blank_indices]\n",
    "            \n",
    "            prob_not_in_any_blank = torch.prod(1 - blank_probs, dim=0)\n",
    "            return 1 - prob_not_in_any_blank\n",
    "\n",
    "    ################################################\n",
    "    # Implemented your \"guess\" function here       #\n",
    "    ################################################\n",
    "    def guess(self, word):\n",
    "        clean_word = word.replace(\" \", \"\")\n",
    "        input_indices = [self.char_to_idx.get(c, self.mask_token_idx) for c in clean_word]\n",
    "        input_tensor = torch.tensor([input_indices], dtype=torch.long).to(self.device)\n",
    "\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        MAX_KERNEL_SIZE = 5 \n",
    "        if seq_len < MAX_KERNEL_SIZE:\n",
    "            padding_needed = MAX_KERNEL_SIZE - seq_len\n",
    "            padding = torch.full((1, padding_needed), self.padding_idx, dtype=torch.long, device=self.device)\n",
    "            input_tensor = torch.cat([input_tensor, padding], dim=1)\n",
    "\n",
    "        # 3. Get predictions from all available models\n",
    "        bilstm_probs = self._get_aggregated_prediction(self.bilstm_model, input_tensor)\n",
    "        transformer_probs = self._get_aggregated_prediction(self.transformer_model, input_tensor)\n",
    "        charcnn_probs = self._get_aggregated_prediction(self.charcnn_model, input_tensor)\n",
    "        \n",
    "        # 4. Combine predictions using ensemble weights\n",
    "        final_probs = torch.zeros(self.vocab_size, device=self.device)\n",
    "        total_weight = 0\n",
    "        \n",
    "        if bilstm_probs is not None:\n",
    "            final_probs += self.W_BILSTM * bilstm_probs\n",
    "            total_weight += self.W_BILSTM\n",
    "        if transformer_probs is not None:\n",
    "            final_probs += self.W_TRANSFORMER * transformer_probs\n",
    "            total_weight += self.W_TRANSFORMER\n",
    "        if charcnn_probs is not None:\n",
    "            final_probs += self.W_CHARCNN * charcnn_probs\n",
    "            total_weight += self.W_CHARCNN\n",
    "\n",
    "        # 5. Handle case where no models are loaded\n",
    "        if total_weight < 1e-5:\n",
    "            print(\"Warning: No models were loaded. Using basic fallback guess.\")\n",
    "            # Fallback to guessing most common letters in English\n",
    "            return next((c for c in \"etaoinshrdlu\" if c not in self.guessed_letters), 'e')\n",
    "\n",
    "        # 6. Normalize probabilities and mask already guessed letters\n",
    "        final_probs /= total_weight\n",
    "        \n",
    "        for letter in self.guessed_letters:\n",
    "            if letter in self.char_to_idx:\n",
    "                final_probs[self.char_to_idx[letter]] = -1.0 # Set to a low value to avoid being picked\n",
    "\n",
    "        # 7. Return the letter with the highest probability\n",
    "        best_guess_idx = torch.argmax(final_probs).item()\n",
    "        return self.idx_to_char[best_guess_idx]\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com']\n",
    "        data = {link: 0 for link in links}\n",
    "        for link in links:\n",
    "            requests.get(link)\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "            \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # Reset guessed letters to empty set\n",
    "        self.guessed_letters = []\n",
    "                                \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully started a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # Get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                        \n",
    "                # Append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessed letters: {}\".format(self.guessed_letters))\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Server response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        if self.access_token:\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models will be loaded onto device: cuda:3\n",
      "--- Initializing Ensemble Hangman Solver ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_358354/3613678437.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_PATH, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded bilstm model from best_bilstm_solver.pth\n",
      "Successfully loaded transformer model from best_transformer_solver.pth\n",
      "Successfully loaded charcnn model from best_charcnn_solver.pth\n",
      "--- Initialization Complete ---\n"
     ]
    }
   ],
   "source": [
    "api = HangmanAPI(access_token=\"70786437cf11ea37b8a7599cccfd20\", timeout=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'Your account has been deactivated!'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m      \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m      time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      4\u001b[0m [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mmy_status() \u001b[38;5;66;03m# Get my game stats: (# of tries, # of wins)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 175\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_game\u001b[39m(\u001b[38;5;28mself\u001b[39m, practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Reset guessed letters to empty set\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 175\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    177\u001b[0m         game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 274\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'Your account has been deactivated!'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "     api.start_game(practice=1,verbose=True)\n",
    "     time.sleep(0.5)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.5f' % (total_practice_runs, practice_success_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  0  th game\n",
      "Playing  1  th game\n",
      "Playing  2  th game\n",
      "Playing  3  th game\n",
      "Playing  4  th game\n",
      "Playing  5  th game\n",
      "Playing  6  th game\n",
      "Playing  7  th game\n",
      "Playing  8  th game\n",
      "Playing  9  th game\n",
      "Playing  10  th game\n",
      "Playing  11  th game\n",
      "Playing  12  th game\n",
      "Playing  13  th game\n",
      "Playing  14  th game\n",
      "Playing  15  th game\n",
      "Playing  16  th game\n",
      "Playing  17  th game\n",
      "Playing  18  th game\n",
      "Playing  19  th game\n",
      "Playing  20  th game\n",
      "Playing  21  th game\n",
      "Playing  22  th game\n",
      "Playing  23  th game\n",
      "Playing  24  th game\n",
      "Playing  25  th game\n",
      "Playing  26  th game\n",
      "Playing  27  th game\n",
      "Playing  28  th game\n",
      "Playing  29  th game\n",
      "Playing  30  th game\n",
      "Playing  31  th game\n",
      "Playing  32  th game\n",
      "Playing  33  th game\n",
      "Playing  34  th game\n",
      "Playing  35  th game\n",
      "Playing  36  th game\n",
      "Playing  37  th game\n",
      "Playing  38  th game\n",
      "Playing  39  th game\n",
      "Playing  40  th game\n",
      "Playing  41  th game\n",
      "Playing  42  th game\n",
      "Playing  43  th game\n",
      "Playing  44  th game\n",
      "Playing  45  th game\n",
      "Playing  46  th game\n",
      "Playing  47  th game\n",
      "Playing  48  th game\n",
      "Playing  49  th game\n",
      "Playing  50  th game\n",
      "Playing  51  th game\n",
      "Playing  52  th game\n",
      "Playing  53  th game\n",
      "Playing  54  th game\n",
      "Playing  55  th game\n",
      "Playing  56  th game\n",
      "Playing  57  th game\n",
      "Playing  58  th game\n",
      "Playing  59  th game\n",
      "Playing  60  th game\n",
      "Playing  61  th game\n",
      "Playing  62  th game\n",
      "Playing  63  th game\n",
      "Playing  64  th game\n",
      "Playing  65  th game\n",
      "Playing  66  th game\n",
      "Playing  67  th game\n",
      "Playing  68  th game\n",
      "Playing  69  th game\n",
      "Playing  70  th game\n",
      "Playing  71  th game\n",
      "Playing  72  th game\n",
      "Playing  73  th game\n",
      "Playing  74  th game\n",
      "Playing  75  th game\n",
      "Playing  76  th game\n",
      "Playing  77  th game\n",
      "Playing  78  th game\n",
      "Playing  79  th game\n",
      "Playing  80  th game\n",
      "Playing  81  th game\n",
      "Playing  82  th game\n",
      "Playing  83  th game\n",
      "Playing  84  th game\n",
      "Playing  85  th game\n",
      "Playing  86  th game\n",
      "Playing  87  th game\n",
      "Playing  88  th game\n",
      "Playing  89  th game\n",
      "Playing  90  th game\n",
      "Playing  91  th game\n",
      "Playing  92  th game\n",
      "Playing  93  th game\n",
      "Playing  94  th game\n",
      "Playing  95  th game\n",
      "Playing  96  th game\n",
      "Playing  97  th game\n",
      "Playing  98  th game\n",
      "Playing  99  th game\n",
      "Playing  100  th game\n",
      "Playing  101  th game\n",
      "Playing  102  th game\n",
      "Playing  103  th game\n",
      "Playing  104  th game\n",
      "Playing  105  th game\n",
      "Playing  106  th game\n",
      "Playing  107  th game\n",
      "Playing  108  th game\n",
      "Playing  109  th game\n",
      "Playing  110  th game\n",
      "Playing  111  th game\n",
      "Playing  112  th game\n",
      "Playing  113  th game\n",
      "Playing  114  th game\n",
      "Playing  115  th game\n",
      "Playing  116  th game\n",
      "Playing  117  th game\n",
      "Playing  118  th game\n",
      "Playing  119  th game\n",
      "Playing  120  th game\n",
      "Playing  121  th game\n",
      "Playing  122  th game\n",
      "Playing  123  th game\n",
      "Playing  124  th game\n",
      "Playing  125  th game\n",
      "Playing  126  th game\n",
      "Playing  127  th game\n",
      "Playing  128  th game\n",
      "Playing  129  th game\n",
      "Playing  130  th game\n",
      "Playing  131  th game\n",
      "Playing  132  th game\n",
      "Playing  133  th game\n",
      "Playing  134  th game\n",
      "Playing  135  th game\n",
      "Playing  136  th game\n",
      "Playing  137  th game\n",
      "Playing  138  th game\n",
      "Playing  139  th game\n",
      "Playing  140  th game\n",
      "Playing  141  th game\n",
      "Playing  142  th game\n",
      "Playing  143  th game\n",
      "Playing  144  th game\n",
      "Playing  145  th game\n",
      "Playing  146  th game\n",
      "Playing  147  th game\n",
      "Playing  148  th game\n",
      "Playing  149  th game\n",
      "Playing  150  th game\n",
      "Playing  151  th game\n",
      "Playing  152  th game\n",
      "Playing  153  th game\n",
      "Playing  154  th game\n",
      "Playing  155  th game\n",
      "Playing  156  th game\n",
      "Playing  157  th game\n",
      "Playing  158  th game\n",
      "Playing  159  th game\n",
      "Playing  160  th game\n",
      "Playing  161  th game\n",
      "Playing  162  th game\n",
      "Playing  163  th game\n",
      "Playing  164  th game\n",
      "Playing  165  th game\n",
      "Playing  166  th game\n",
      "Playing  167  th game\n",
      "Playing  168  th game\n",
      "Playing  169  th game\n",
      "Playing  170  th game\n",
      "Playing  171  th game\n",
      "Playing  172  th game\n",
      "Playing  173  th game\n",
      "Playing  174  th game\n",
      "Playing  175  th game\n",
      "Playing  176  th game\n",
      "Playing  177  th game\n",
      "Playing  178  th game\n",
      "Playing  179  th game\n",
      "Playing  180  th game\n",
      "Playing  181  th game\n",
      "Playing  182  th game\n",
      "Playing  183  th game\n",
      "Playing  184  th game\n",
      "Playing  185  th game\n",
      "Playing  186  th game\n",
      "Playing  187  th game\n",
      "Playing  188  th game\n",
      "Playing  189  th game\n",
      "Playing  190  th game\n",
      "Playing  191  th game\n",
      "Playing  192  th game\n",
      "Playing  193  th game\n",
      "Playing  194  th game\n",
      "Playing  195  th game\n",
      "Playing  196  th game\n",
      "Playing  197  th game\n",
      "Playing  198  th game\n",
      "Playing  199  th game\n",
      "Playing  200  th game\n",
      "Playing  201  th game\n",
      "Playing  202  th game\n",
      "Playing  203  th game\n",
      "Playing  204  th game\n",
      "Playing  205  th game\n",
      "Playing  206  th game\n",
      "Playing  207  th game\n",
      "Playing  208  th game\n",
      "Playing  209  th game\n",
      "Playing  210  th game\n",
      "Playing  211  th game\n",
      "Playing  212  th game\n",
      "Playing  213  th game\n",
      "Playing  214  th game\n",
      "Playing  215  th game\n",
      "Playing  216  th game\n",
      "Playing  217  th game\n",
      "Playing  218  th game\n",
      "Playing  219  th game\n",
      "Playing  220  th game\n",
      "Playing  221  th game\n",
      "Playing  222  th game\n",
      "Playing  223  th game\n",
      "Playing  224  th game\n",
      "Playing  225  th game\n",
      "Playing  226  th game\n",
      "Playing  227  th game\n",
      "Playing  228  th game\n",
      "Playing  229  th game\n",
      "Playing  230  th game\n",
      "Playing  231  th game\n",
      "Playing  232  th game\n",
      "Playing  233  th game\n",
      "Playing  234  th game\n",
      "Playing  235  th game\n",
      "Playing  236  th game\n",
      "Playing  237  th game\n",
      "Playing  238  th game\n",
      "Playing  239  th game\n",
      "Playing  240  th game\n",
      "Playing  241  th game\n",
      "Playing  242  th game\n",
      "Playing  243  th game\n",
      "Playing  244  th game\n",
      "Playing  245  th game\n",
      "Playing  246  th game\n",
      "Playing  247  th game\n",
      "Playing  248  th game\n",
      "Playing  249  th game\n",
      "Playing  250  th game\n",
      "Playing  251  th game\n",
      "Playing  252  th game\n",
      "Playing  253  th game\n",
      "Playing  254  th game\n",
      "Playing  255  th game\n",
      "Playing  256  th game\n",
      "Playing  257  th game\n",
      "Playing  258  th game\n",
      "Playing  259  th game\n",
      "Playing  260  th game\n",
      "Playing  261  th game\n",
      "Playing  262  th game\n",
      "Playing  263  th game\n",
      "Playing  264  th game\n",
      "Playing  265  th game\n",
      "Playing  266  th game\n",
      "Playing  267  th game\n",
      "Playing  268  th game\n",
      "Playing  269  th game\n",
      "Playing  270  th game\n",
      "Playing  271  th game\n",
      "Playing  272  th game\n",
      "Playing  273  th game\n",
      "Playing  274  th game\n",
      "Playing  275  th game\n",
      "Playing  276  th game\n",
      "Playing  277  th game\n",
      "Playing  278  th game\n",
      "Playing  279  th game\n",
      "Playing  280  th game\n",
      "Playing  281  th game\n",
      "Playing  282  th game\n",
      "Playing  283  th game\n",
      "Playing  284  th game\n",
      "Playing  285  th game\n",
      "Playing  286  th game\n",
      "Playing  287  th game\n",
      "Playing  288  th game\n",
      "Playing  289  th game\n",
      "Playing  290  th game\n",
      "Playing  291  th game\n",
      "Playing  292  th game\n",
      "Playing  293  th game\n",
      "Playing  294  th game\n",
      "Playing  295  th game\n",
      "Playing  296  th game\n",
      "Playing  297  th game\n",
      "Playing  298  th game\n",
      "Playing  299  th game\n",
      "Playing  300  th game\n",
      "Playing  301  th game\n",
      "Playing  302  th game\n",
      "Playing  303  th game\n",
      "Playing  304  th game\n",
      "Playing  305  th game\n",
      "Playing  306  th game\n",
      "Playing  307  th game\n",
      "Playing  308  th game\n",
      "Playing  309  th game\n",
      "Playing  310  th game\n",
      "Playing  311  th game\n",
      "Playing  312  th game\n",
      "Playing  313  th game\n",
      "Playing  314  th game\n",
      "Playing  315  th game\n",
      "Playing  316  th game\n",
      "Playing  317  th game\n",
      "Playing  318  th game\n",
      "Playing  319  th game\n",
      "Playing  320  th game\n",
      "Playing  321  th game\n",
      "Playing  322  th game\n",
      "Playing  323  th game\n",
      "Playing  324  th game\n",
      "Playing  325  th game\n",
      "Playing  326  th game\n",
      "Playing  327  th game\n",
      "Playing  328  th game\n",
      "Playing  329  th game\n",
      "Playing  330  th game\n",
      "Playing  331  th game\n",
      "Playing  332  th game\n",
      "Playing  333  th game\n",
      "Playing  334  th game\n",
      "Playing  335  th game\n",
      "Playing  336  th game\n",
      "Playing  337  th game\n",
      "Playing  338  th game\n",
      "Playing  339  th game\n",
      "Playing  340  th game\n",
      "Playing  341  th game\n",
      "Playing  342  th game\n",
      "Playing  343  th game\n",
      "Playing  344  th game\n",
      "Playing  345  th game\n",
      "Playing  346  th game\n",
      "Playing  347  th game\n",
      "Playing  348  th game\n",
      "Playing  349  th game\n",
      "Playing  350  th game\n",
      "Playing  351  th game\n",
      "Playing  352  th game\n",
      "Playing  353  th game\n",
      "Playing  354  th game\n",
      "Playing  355  th game\n",
      "Playing  356  th game\n",
      "Playing  357  th game\n",
      "Playing  358  th game\n",
      "Playing  359  th game\n",
      "Playing  360  th game\n",
      "Playing  361  th game\n",
      "Playing  362  th game\n",
      "Playing  363  th game\n",
      "Playing  364  th game\n",
      "Playing  365  th game\n",
      "Playing  366  th game\n",
      "Playing  367  th game\n",
      "Playing  368  th game\n",
      "Playing  369  th game\n",
      "Playing  370  th game\n",
      "Playing  371  th game\n",
      "Playing  372  th game\n",
      "Playing  373  th game\n",
      "Playing  374  th game\n",
      "Playing  375  th game\n",
      "Playing  376  th game\n",
      "Playing  377  th game\n",
      "Playing  378  th game\n",
      "Playing  379  th game\n",
      "Playing  380  th game\n",
      "Playing  381  th game\n",
      "Playing  382  th game\n",
      "Playing  383  th game\n",
      "Playing  384  th game\n",
      "Playing  385  th game\n",
      "Playing  386  th game\n",
      "Playing  387  th game\n",
      "Playing  388  th game\n",
      "Playing  389  th game\n",
      "Playing  390  th game\n",
      "Playing  391  th game\n",
      "Playing  392  th game\n",
      "Playing  393  th game\n",
      "Playing  394  th game\n",
      "Playing  395  th game\n",
      "Playing  396  th game\n",
      "Playing  397  th game\n",
      "Playing  398  th game\n",
      "Playing  399  th game\n",
      "Playing  400  th game\n",
      "Playing  401  th game\n",
      "Playing  402  th game\n",
      "Playing  403  th game\n",
      "Playing  404  th game\n",
      "Playing  405  th game\n",
      "Playing  406  th game\n",
      "Playing  407  th game\n",
      "Playing  408  th game\n",
      "Playing  409  th game\n",
      "Playing  410  th game\n",
      "Playing  411  th game\n",
      "Playing  412  th game\n",
      "Playing  413  th game\n",
      "Playing  414  th game\n",
      "Playing  415  th game\n",
      "Playing  416  th game\n",
      "Playing  417  th game\n",
      "Playing  418  th game\n",
      "Playing  419  th game\n",
      "Playing  420  th game\n",
      "Playing  421  th game\n",
      "Playing  422  th game\n",
      "Playing  423  th game\n",
      "Playing  424  th game\n",
      "Playing  425  th game\n",
      "Playing  426  th game\n",
      "Playing  427  th game\n",
      "Playing  428  th game\n",
      "Playing  429  th game\n",
      "Playing  430  th game\n",
      "Playing  431  th game\n",
      "Playing  432  th game\n",
      "Playing  433  th game\n",
      "Playing  434  th game\n",
      "Playing  435  th game\n",
      "Playing  436  th game\n",
      "Playing  437  th game\n",
      "Playing  438  th game\n",
      "Playing  439  th game\n",
      "Playing  440  th game\n",
      "Playing  441  th game\n",
      "Playing  442  th game\n",
      "Playing  443  th game\n",
      "Playing  444  th game\n",
      "Playing  445  th game\n",
      "Playing  446  th game\n",
      "Playing  447  th game\n",
      "Playing  448  th game\n",
      "Playing  449  th game\n",
      "Playing  450  th game\n",
      "Playing  451  th game\n",
      "Playing  452  th game\n",
      "Playing  453  th game\n",
      "Playing  454  th game\n",
      "Playing  455  th game\n",
      "Playing  456  th game\n",
      "Playing  457  th game\n",
      "Playing  458  th game\n",
      "Playing  459  th game\n",
      "Playing  460  th game\n",
      "Playing  461  th game\n",
      "Playing  462  th game\n",
      "Playing  463  th game\n",
      "Playing  464  th game\n",
      "Playing  465  th game\n",
      "Playing  466  th game\n",
      "Playing  467  th game\n",
      "Playing  468  th game\n",
      "Playing  469  th game\n",
      "Playing  470  th game\n",
      "Playing  471  th game\n",
      "Playing  472  th game\n",
      "Playing  473  th game\n",
      "Playing  474  th game\n",
      "Playing  475  th game\n",
      "Playing  476  th game\n",
      "Playing  477  th game\n",
      "Playing  478  th game\n",
      "Playing  479  th game\n",
      "Playing  480  th game\n",
      "Playing  481  th game\n",
      "Playing  482  th game\n",
      "Playing  483  th game\n",
      "Playing  484  th game\n",
      "Playing  485  th game\n",
      "Playing  486  th game\n",
      "Playing  487  th game\n",
      "Playing  488  th game\n",
      "Playing  489  th game\n",
      "Playing  490  th game\n",
      "Playing  491  th game\n",
      "Playing  492  th game\n",
      "Playing  493  th game\n",
      "Playing  494  th game\n",
      "Playing  495  th game\n",
      "Playing  496  th game\n",
      "Playing  497  th game\n",
      "Playing  498  th game\n",
      "Playing  499  th game\n",
      "Playing  500  th game\n",
      "Playing  501  th game\n",
      "Playing  502  th game\n",
      "Playing  503  th game\n",
      "Playing  504  th game\n",
      "Playing  505  th game\n",
      "Playing  506  th game\n",
      "Playing  507  th game\n",
      "Playing  508  th game\n",
      "Playing  509  th game\n",
      "Playing  510  th game\n",
      "Playing  511  th game\n",
      "Playing  512  th game\n",
      "Playing  513  th game\n",
      "Playing  514  th game\n",
      "Playing  515  th game\n",
      "Playing  516  th game\n",
      "Playing  517  th game\n",
      "Playing  518  th game\n",
      "Playing  519  th game\n",
      "Playing  520  th game\n",
      "Playing  521  th game\n",
      "Playing  522  th game\n",
      "Playing  523  th game\n",
      "Playing  524  th game\n",
      "Playing  525  th game\n",
      "Playing  526  th game\n",
      "Playing  527  th game\n",
      "Playing  528  th game\n",
      "Playing  529  th game\n",
      "Playing  530  th game\n",
      "Playing  531  th game\n",
      "Playing  532  th game\n",
      "Playing  533  th game\n",
      "Playing  534  th game\n",
      "Playing  535  th game\n",
      "Playing  536  th game\n",
      "Playing  537  th game\n",
      "Playing  538  th game\n",
      "Playing  539  th game\n",
      "Playing  540  th game\n",
      "Playing  541  th game\n",
      "Playing  542  th game\n",
      "Playing  543  th game\n",
      "Playing  544  th game\n",
      "Playing  545  th game\n",
      "Playing  546  th game\n",
      "Playing  547  th game\n",
      "Playing  548  th game\n",
      "Playing  549  th game\n",
      "Playing  550  th game\n",
      "Playing  551  th game\n",
      "Playing  552  th game\n",
      "Playing  553  th game\n",
      "Playing  554  th game\n",
      "Playing  555  th game\n",
      "Playing  556  th game\n",
      "Playing  557  th game\n",
      "Playing  558  th game\n",
      "Playing  559  th game\n",
      "Playing  560  th game\n",
      "Playing  561  th game\n",
      "Playing  562  th game\n",
      "Playing  563  th game\n",
      "Playing  564  th game\n",
      "Playing  565  th game\n",
      "Playing  566  th game\n",
      "Playing  567  th game\n",
      "Playing  568  th game\n",
      "Playing  569  th game\n",
      "Playing  570  th game\n",
      "Playing  571  th game\n",
      "Playing  572  th game\n",
      "Playing  573  th game\n",
      "Playing  574  th game\n",
      "Playing  575  th game\n",
      "Playing  576  th game\n",
      "Playing  577  th game\n",
      "Playing  578  th game\n",
      "Playing  579  th game\n",
      "Playing  580  th game\n",
      "Playing  581  th game\n",
      "Playing  582  th game\n",
      "Playing  583  th game\n",
      "Playing  584  th game\n",
      "Playing  585  th game\n",
      "Playing  586  th game\n",
      "Playing  587  th game\n",
      "Playing  588  th game\n",
      "Playing  589  th game\n",
      "Playing  590  th game\n",
      "Playing  591  th game\n",
      "Playing  592  th game\n",
      "Playing  593  th game\n",
      "Playing  594  th game\n",
      "Playing  595  th game\n",
      "Playing  596  th game\n",
      "Playing  597  th game\n",
      "Playing  598  th game\n",
      "Playing  599  th game\n",
      "Playing  600  th game\n",
      "Playing  601  th game\n",
      "Playing  602  th game\n",
      "Playing  603  th game\n",
      "Playing  604  th game\n",
      "Playing  605  th game\n",
      "Playing  606  th game\n",
      "Playing  607  th game\n",
      "Playing  608  th game\n",
      "Playing  609  th game\n",
      "Playing  610  th game\n",
      "Playing  611  th game\n",
      "Playing  612  th game\n",
      "Playing  613  th game\n",
      "Playing  614  th game\n",
      "Playing  615  th game\n",
      "Playing  616  th game\n",
      "Playing  617  th game\n",
      "Playing  618  th game\n",
      "Playing  619  th game\n",
      "Playing  620  th game\n",
      "Playing  621  th game\n",
      "Playing  622  th game\n",
      "Playing  623  th game\n",
      "Playing  624  th game\n",
      "Playing  625  th game\n",
      "Playing  626  th game\n",
      "Playing  627  th game\n",
      "Playing  628  th game\n",
      "Playing  629  th game\n",
      "Playing  630  th game\n",
      "Playing  631  th game\n",
      "Playing  632  th game\n",
      "Playing  633  th game\n",
      "Playing  634  th game\n",
      "Playing  635  th game\n",
      "Playing  636  th game\n",
      "Playing  637  th game\n",
      "Playing  638  th game\n",
      "Playing  639  th game\n",
      "Playing  640  th game\n",
      "Playing  641  th game\n",
      "Playing  642  th game\n",
      "Playing  643  th game\n",
      "Playing  644  th game\n",
      "Playing  645  th game\n",
      "Playing  646  th game\n",
      "Playing  647  th game\n",
      "Playing  648  th game\n",
      "Playing  649  th game\n",
      "Playing  650  th game\n",
      "Playing  651  th game\n",
      "Playing  652  th game\n",
      "Playing  653  th game\n",
      "Playing  654  th game\n",
      "Playing  655  th game\n",
      "Playing  656  th game\n",
      "Playing  657  th game\n",
      "Playing  658  th game\n",
      "Playing  659  th game\n",
      "Playing  660  th game\n",
      "Playing  661  th game\n",
      "Playing  662  th game\n",
      "Playing  663  th game\n",
      "Playing  664  th game\n",
      "Playing  665  th game\n",
      "Playing  666  th game\n",
      "Playing  667  th game\n",
      "Playing  668  th game\n",
      "Playing  669  th game\n",
      "Playing  670  th game\n",
      "Playing  671  th game\n",
      "Playing  672  th game\n",
      "Playing  673  th game\n",
      "Playing  674  th game\n",
      "Playing  675  th game\n",
      "Playing  676  th game\n",
      "Playing  677  th game\n",
      "Playing  678  th game\n",
      "Playing  679  th game\n",
      "Playing  680  th game\n",
      "Playing  681  th game\n",
      "Playing  682  th game\n",
      "Playing  683  th game\n",
      "Playing  684  th game\n",
      "Playing  685  th game\n",
      "Playing  686  th game\n",
      "Playing  687  th game\n",
      "Playing  688  th game\n",
      "Playing  689  th game\n",
      "Playing  690  th game\n",
      "Playing  691  th game\n",
      "Playing  692  th game\n",
      "Playing  693  th game\n",
      "Playing  694  th game\n",
      "Playing  695  th game\n",
      "Playing  696  th game\n",
      "Playing  697  th game\n",
      "Playing  698  th game\n",
      "Playing  699  th game\n",
      "Playing  700  th game\n",
      "Playing  701  th game\n",
      "Playing  702  th game\n",
      "Playing  703  th game\n",
      "Playing  704  th game\n",
      "Playing  705  th game\n",
      "Playing  706  th game\n",
      "Playing  707  th game\n",
      "Playing  708  th game\n",
      "Playing  709  th game\n",
      "Playing  710  th game\n",
      "Playing  711  th game\n",
      "Playing  712  th game\n",
      "Playing  713  th game\n",
      "Playing  714  th game\n",
      "Playing  715  th game\n",
      "Playing  716  th game\n",
      "Playing  717  th game\n",
      "Playing  718  th game\n",
      "Playing  719  th game\n",
      "Playing  720  th game\n",
      "Playing  721  th game\n",
      "Playing  722  th game\n",
      "Playing  723  th game\n",
      "Playing  724  th game\n",
      "Playing  725  th game\n",
      "Playing  726  th game\n",
      "Playing  727  th game\n",
      "Playing  728  th game\n",
      "Playing  729  th game\n",
      "Playing  730  th game\n",
      "Playing  731  th game\n",
      "Playing  732  th game\n",
      "Playing  733  th game\n",
      "Playing  734  th game\n",
      "Playing  735  th game\n",
      "Playing  736  th game\n",
      "Playing  737  th game\n",
      "Playing  738  th game\n",
      "Playing  739  th game\n",
      "Playing  740  th game\n",
      "Playing  741  th game\n",
      "Playing  742  th game\n",
      "Playing  743  th game\n",
      "Playing  744  th game\n",
      "Playing  745  th game\n",
      "Playing  746  th game\n",
      "Playing  747  th game\n",
      "Playing  748  th game\n",
      "Playing  749  th game\n",
      "Playing  750  th game\n",
      "Playing  751  th game\n",
      "Playing  752  th game\n",
      "Playing  753  th game\n",
      "Playing  754  th game\n",
      "Playing  755  th game\n",
      "Playing  756  th game\n",
      "Playing  757  th game\n",
      "Playing  758  th game\n",
      "Playing  759  th game\n",
      "Playing  760  th game\n",
      "Playing  761  th game\n",
      "Playing  762  th game\n",
      "Playing  763  th game\n",
      "Playing  764  th game\n",
      "Playing  765  th game\n",
      "Playing  766  th game\n",
      "Playing  767  th game\n",
      "Playing  768  th game\n",
      "Playing  769  th game\n",
      "Playing  770  th game\n",
      "Playing  771  th game\n",
      "Playing  772  th game\n",
      "Playing  773  th game\n",
      "Playing  774  th game\n",
      "Playing  775  th game\n",
      "Playing  776  th game\n",
      "Playing  777  th game\n",
      "Playing  778  th game\n",
      "Playing  779  th game\n",
      "Playing  780  th game\n",
      "Playing  781  th game\n",
      "Playing  782  th game\n",
      "Playing  783  th game\n",
      "Playing  784  th game\n",
      "Playing  785  th game\n",
      "Playing  786  th game\n",
      "Playing  787  th game\n",
      "Playing  788  th game\n",
      "Playing  789  th game\n",
      "Playing  790  th game\n",
      "Playing  791  th game\n",
      "Playing  792  th game\n",
      "Playing  793  th game\n",
      "Playing  794  th game\n",
      "Playing  795  th game\n",
      "Playing  796  th game\n",
      "Playing  797  th game\n",
      "Playing  798  th game\n",
      "Playing  799  th game\n",
      "Playing  800  th game\n",
      "Playing  801  th game\n",
      "Playing  802  th game\n",
      "Playing  803  th game\n",
      "Playing  804  th game\n",
      "Playing  805  th game\n",
      "Playing  806  th game\n",
      "Playing  807  th game\n",
      "Playing  808  th game\n",
      "Playing  809  th game\n",
      "Playing  810  th game\n",
      "Playing  811  th game\n",
      "Playing  812  th game\n",
      "Playing  813  th game\n",
      "Playing  814  th game\n",
      "Playing  815  th game\n",
      "Playing  816  th game\n",
      "Playing  817  th game\n",
      "Playing  818  th game\n",
      "Playing  819  th game\n",
      "Playing  820  th game\n",
      "Playing  821  th game\n",
      "Playing  822  th game\n",
      "Playing  823  th game\n",
      "Playing  824  th game\n",
      "Playing  825  th game\n",
      "Playing  826  th game\n",
      "Playing  827  th game\n",
      "Playing  828  th game\n",
      "Playing  829  th game\n",
      "Playing  830  th game\n",
      "Playing  831  th game\n",
      "Playing  832  th game\n",
      "Playing  833  th game\n",
      "Playing  834  th game\n",
      "Playing  835  th game\n",
      "Playing  836  th game\n",
      "Playing  837  th game\n",
      "Playing  838  th game\n",
      "Playing  839  th game\n",
      "Playing  840  th game\n",
      "Playing  841  th game\n",
      "Playing  842  th game\n",
      "Playing  843  th game\n",
      "Playing  844  th game\n",
      "Playing  845  th game\n",
      "Playing  846  th game\n",
      "Playing  847  th game\n",
      "Playing  848  th game\n",
      "Playing  849  th game\n",
      "Playing  850  th game\n",
      "Playing  851  th game\n",
      "Playing  852  th game\n",
      "Playing  853  th game\n",
      "Playing  854  th game\n",
      "Playing  855  th game\n",
      "Playing  856  th game\n",
      "Playing  857  th game\n",
      "Playing  858  th game\n",
      "Playing  859  th game\n",
      "Playing  860  th game\n",
      "Playing  861  th game\n",
      "Playing  862  th game\n",
      "Playing  863  th game\n",
      "Playing  864  th game\n",
      "Playing  865  th game\n",
      "Playing  866  th game\n",
      "Playing  867  th game\n",
      "Playing  868  th game\n",
      "Playing  869  th game\n",
      "Playing  870  th game\n",
      "Playing  871  th game\n",
      "Playing  872  th game\n",
      "Playing  873  th game\n",
      "Playing  874  th game\n",
      "Playing  875  th game\n",
      "Playing  876  th game\n",
      "Playing  877  th game\n",
      "Playing  878  th game\n",
      "Playing  879  th game\n",
      "Playing  880  th game\n",
      "Playing  881  th game\n",
      "Playing  882  th game\n",
      "Playing  883  th game\n",
      "Playing  884  th game\n",
      "Playing  885  th game\n",
      "Playing  886  th game\n",
      "Playing  887  th game\n",
      "Playing  888  th game\n",
      "Playing  889  th game\n",
      "Playing  890  th game\n",
      "Playing  891  th game\n",
      "Playing  892  th game\n",
      "Playing  893  th game\n",
      "Playing  894  th game\n",
      "Playing  895  th game\n",
      "Playing  896  th game\n",
      "Playing  897  th game\n",
      "Playing  898  th game\n",
      "Playing  899  th game\n",
      "Playing  900  th game\n",
      "Playing  901  th game\n",
      "Playing  902  th game\n",
      "Playing  903  th game\n",
      "Playing  904  th game\n",
      "Playing  905  th game\n",
      "Playing  906  th game\n",
      "Playing  907  th game\n",
      "Playing  908  th game\n",
      "Playing  909  th game\n",
      "Playing  910  th game\n",
      "Playing  911  th game\n",
      "Playing  912  th game\n",
      "Playing  913  th game\n",
      "Playing  914  th game\n",
      "Playing  915  th game\n",
      "Playing  916  th game\n",
      "Playing  917  th game\n",
      "Playing  918  th game\n",
      "Playing  919  th game\n",
      "Playing  920  th game\n",
      "Playing  921  th game\n",
      "Playing  922  th game\n",
      "Playing  923  th game\n",
      "Playing  924  th game\n",
      "Playing  925  th game\n",
      "Playing  926  th game\n",
      "Playing  927  th game\n",
      "Playing  928  th game\n",
      "Playing  929  th game\n",
      "Playing  930  th game\n",
      "Playing  931  th game\n",
      "Playing  932  th game\n",
      "Playing  933  th game\n",
      "Playing  934  th game\n",
      "Playing  935  th game\n",
      "Playing  936  th game\n",
      "Playing  937  th game\n",
      "Playing  938  th game\n",
      "Playing  939  th game\n",
      "Playing  940  th game\n",
      "Playing  941  th game\n",
      "Playing  942  th game\n",
      "Playing  943  th game\n",
      "Playing  944  th game\n",
      "Playing  945  th game\n",
      "Playing  946  th game\n",
      "Playing  947  th game\n",
      "Playing  948  th game\n",
      "Playing  949  th game\n",
      "Playing  950  th game\n",
      "Playing  951  th game\n",
      "Playing  952  th game\n",
      "Playing  953  th game\n",
      "Playing  954  th game\n",
      "Playing  955  th game\n",
      "Playing  956  th game\n",
      "Playing  957  th game\n",
      "Playing  958  th game\n",
      "Playing  959  th game\n",
      "Playing  960  th game\n",
      "Playing  961  th game\n",
      "Playing  962  th game\n",
      "Playing  963  th game\n",
      "Playing  964  th game\n",
      "Playing  965  th game\n",
      "Playing  966  th game\n",
      "Playing  967  th game\n",
      "Playing  968  th game\n",
      "Playing  969  th game\n",
      "Playing  970  th game\n",
      "Playing  971  th game\n",
      "Playing  972  th game\n",
      "Playing  973  th game\n",
      "Playing  974  th game\n",
      "Playing  975  th game\n",
      "Playing  976  th game\n",
      "Playing  977  th game\n",
      "Playing  978  th game\n",
      "Playing  979  th game\n",
      "Playing  980  th game\n",
      "Playing  981  th game\n",
      "Playing  982  th game\n",
      "Playing  983  th game\n",
      "Playing  984  th game\n",
      "Playing  985  th game\n",
      "Playing  986  th game\n",
      "Playing  987  th game\n",
      "Playing  988  th game\n",
      "Playing  989  th game\n",
      "Playing  990  th game\n",
      "Playing  991  th game\n",
      "Playing  992  th game\n",
      "Playing  993  th game\n",
      "Playing  994  th game\n",
      "Playing  995  th game\n",
      "Playing  996  th game\n",
      "Playing  997  th game\n",
      "Playing  998  th game\n",
      "Playing  999  th game\n"
     ]
    },
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[39;00m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 175\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_game\u001b[39m(\u001b[38;5;28mself\u001b[39m, practice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Reset guessed letters to empty set\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 175\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    177\u001b[0m         game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 274\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.60100\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.5f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hangman_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
